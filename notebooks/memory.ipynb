{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory usage estimation\n",
    "\n",
    "This file extends the memory usage calculations within the playground notebook for specific tasks. \n",
    "\n",
    "## Individual radial profiles for halos\n",
    "\n",
    "We start by estimating the memory use at different stages when calculating the temperature radial profiles for individual halos, taking into account not just FoF particles, but all particles within $2 R_{vir}$. The steps are as follows:\n",
    "\n",
    "1. Load halo data and restrict it to the 280 halos that have $M > 10^{14}M_\\odot$.\n",
    "2. Load the three fields of all gas particles required for temperature\n",
    "3. Calculate temperature, discard gas data\n",
    "4. Load positions of all gas particles (optionally: construct a KD-tree)\n",
    "5. For a single halo: find particles within sphere of radius $r = 2R_{vir}$ by creating a mask over all particles\n",
    "6. Calculate radial distance of all particles within the sphere to the halo center\n",
    "7. Mask particle arrays of temperature and position\n",
    "8. Calculate the histogram and save it to file\n",
    "\n",
    "The memory usage will be calculated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "import h5py\n",
    "import logging\n",
    "import logging.config\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import illustris_python as il\n",
    "\n",
    "# import the helper scripts\n",
    "module_path = Path(os.getcwd()) / \"..\" / \"src\"\n",
    "sys.path.append(str(module_path.resolve()))\n",
    "from library.config import logging_config, config\n",
    "from library import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: I am a test log!\n"
     ]
    }
   ],
   "source": [
    "logging_cfg = logging_config.get_logging_config(\"INFO\")\n",
    "logging.config.dictConfig(logging_cfg)\n",
    "logger = logging.getLogger(\"root\")\n",
    "# test setup\n",
    "logger.info(\"I am a test log!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and restrict halo data\n",
    "\n",
    "We load the position and mass of 280 halos, resulting in 1 `float32` and 3 `float64` numbers per halo. As such, we can expect to use up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_memory_restricted_halos():\n",
    "    positions = np.zeros((280, 3), dtype=np.float64)\n",
    "    masses = np.zeros(280, dtype=np.float32)\n",
    "    size = sys.getsizeof(positions) + sys.getsizeof(masses)\n",
    "    return size  # in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated size of restricted halos: 7.890625 kB\n"
     ]
    }
   ],
   "source": [
    "st1_estimate = estimate_memory_restricted_halos() / 1024\n",
    "print(f\"Estimated size of restricted halos: {st1_estimate} kB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to other data being stored, the actual memory used after this step will be much larger (think configuration variables, the pipeline object, etc.). It is found to actually be:\n",
    "\n",
    "**Actual memory use after step 1:** 85.9 kB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load three fields for temperature calculation\n",
    "\n",
    "We now load the fields `InternalEnergy`, `ElectronAbundance`, and `StarFormationRate` to calculate temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_particles():\n",
    "    cfg = config.get_default_config(\"TNG300-1\")\n",
    "    f = h5py.File(il.snapshot.snapPath(cfg.base_path, cfg.snap_num, 0), 'r')\n",
    "    n_part = f[\"Header\"].attrs[\"NumPart_Total\"][0]\n",
    "    n_part += f[\"Header\"].attrs[\"NumPart_Total_HighWord\"][0] * 2**32\n",
    "    return n_part\n",
    "\n",
    "\n",
    "def estimate_memory_gas_data():\n",
    "    n_parts = get_number_particles()\n",
    "    return 3 * 32 * n_parts / 8  # in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated size of gas data: 161.54410924762487 GB\n"
     ]
    }
   ],
   "source": [
    "st2_estimate = estimate_memory_gas_data() / 1024 / 1024 / 1024\n",
    "print(f\"Estimated size of gas data: {st2_estimate} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches the measured memory used well: **actual memory use** is measured to be 161.5 GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculating temperatures\n",
    "\n",
    "The peak memory usage during the calculation is difficult to estimate and depends strongly on the method. Using `numpy` and letting it optimize the calculation leads to both the best-case runtime and memory usage of ~175.1 GB of peak memory used for calculation and allocated result array. This leads to a peak memory use of 336.6 GB (measured).\n",
    "\n",
    "After this step, clean-up reduces the space in memory required to purely the temperature array plus the previouslz allocated memory for halo data and pipeline config. This leads to an estimate of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory use after cleaning up: 53.84803641587496 GB.\n"
     ]
    }
   ],
   "source": [
    "st3_estimate = get_number_particles() * 32 / 8  / 1024 / 1024 / 1024\n",
    "print(f\"Estimated memory use after cleaning up: {st3_estimate} GB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this matches well with the **measured memory use** after clean-up, which comes out to 53.85 GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Loading position data of all gas cells\n",
    "\n",
    "This is by far the most memory-intensive step, unless the position data is cast to `float32` - it is, by default, loaded as `float64` data. If one casts down to `float32`, then the memory use of the position data is equivalent to that of the gas cell data, i.e. 161.5 GB.\n",
    "\n",
    "If we use the `float64` data instead, we come out at 323 GB memory use - too much considering the memory-intensive calculations we expect to follow up with. Casting to `float32` seems sensible. \n",
    "\n",
    "In addition to this, the temperature data persists, leading to a total memory use after loading position data of:\n",
    "\n",
    "- `float32` case: 235.45 GB\n",
    "- `float64` case: 417.05 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "illustris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
